---
title: 线性回归
author: ' curve'
date: '2018-04-06'
slug: ''
categories:
  - R
tags:
  - Linear regression
---

(未完成！)

作为统计学的支柱，线性回归有着不可替代的作用。同时线性模型同时还拥有着许多优点：

1. 简单，并可以提供充分可解释的的描述
2. 在面对缺少足够的数据的训练集、低信噪比、稀疏数据时线性模型比一些新奇的非线性模型要优秀。
3. 对输入变量的变化，可扩大其使用范围。
4. 许多非线性模式是线性模型的推广，线性模型是其他模型的基础。

相比其他模型，线性模型似乎是一种简单的模型。但是想要了解透线性模型的方方面面绝非是一件容易的事情。 
  
作为一个实用主义者，我们将通过 R 语言完成线性模型建模的一套流程，并在其中探讨一些细节。
  
首先，我们认为输入变量 $X$ 和输出变量 $Y$ 在现实生活中有着明确的线性关系。

$$
Y=f(X)+\epsilon
$$

其中 $\epsilon$ 为随机误差项，且均值为 0。这代表着，我们的模型是一个概率模型。其有着确定的部分（$f(X)$）和不确定的部分（$\epsilon$）。现实生活中自变量与因变量之间的关系往往是不确定的，因此我们需要用数据估计出：

$$
\hat{Y}=\hat{f}(X)
$$

对应线性模型即为：

$$
\hat { Y } = \hat { \beta } _ { 0} + \hat { \beta } _ { 1} X _ { 1 } + \dots + \hat { \beta } _ { k } X _ { k  } 
$$

----

## 多元回归中的一些重要问题

在做多元回归的时候我们经常会面对如下一些问题：

1. 预测变量中是否是否至少有一个可以用来预测响应变量
2. 所有预测变量都有助于理解 Y 吗？或仅仅是预测变量的一个子集对预测有用？
3. 模型对数据拟合程度如何？
4. 给定一组预测变量的值，响应值应预测为多少？多做预测的准确程度如何。

### F test

如我们在一元回归中要验证原假设 $\beta_1 = 0$ 一样 （在一元线性回归中由于只有一个系数，因此模型的 F 检验等于等同于相应系数的 t 检验）多元回归中模型的原假设则为 $\beta_1=\beta_2=\dots=\beta_p=0$. 备择假设为：至少有一个 $\beta_j$ 不为零。
  
同样我们需要计算 F 统计量：

$$
F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}
$$

如果原假设正确，则：

$$
E\{RSS/(n-p-1)\}=\sigma^2\\
E\{(TSS-RSS)/p\}=\sigma^2
$$

因此，若预测变量与响应变量无关时，F 统计量应该是接近 1 的。在误差项  $\sigma_j$ 满足正态分布的时候，F 统计量满足 F 分布。我们可以通过对应的 p 值来做判断。

























