---
title: 线性回归
author: ' curve'
date: '2018-04-06'
slug: ''
categories:
  - R
tags:
  - Linear regression
---

(未完成！)

作为统计学的支柱，线性回归有着不可替代的作用。同时线性模型同时还拥有着许多优点：

1. 简单，并可以提供充分可解释的的描述
2. 在面对缺少足够的数据的训练集、低信噪比、稀疏数据时线性模型比一些新奇的非线性模型要优秀。
3. 对输入变量的变化，可扩大其使用范围。
4. 许多非线性模式是线性模型的推广，线性模型是其他模型的基础。

相比其他模型，线性模型似乎是一种简单的模型。但是想要了解透线性模型的方方面面绝非是一件容易的事情。 
  
作为一个实用主义者，我们将通过 R 语言完成线性模型建模的一套流程，并在其中探讨一些细节。
  
首先，我们认为输入变量 $X$ 和输出变量 $Y$ 在现实生活中有着明确的线性关系。

$$
Y=f(X)+\epsilon
$$

其中 $\epsilon$ 为随机误差项，且均值为 0。这代表着，我们的模型是一个概率模型。其有着确定的部分（$f(X)$）和不确定的部分（$\epsilon$）。现实生活中自变量与因变量之间的关系往往是不确定的，因此我们需要用数据估计出：

$$
\hat{Y}=\hat{f}(X)
$$

对应线性模型即为：

$$
\hat { Y } = \hat { \beta } _ { 0} + \hat { \beta } _ { 1} X _ { 1 } + \dots + \hat { \beta } _ { k } X _ { k  } 
$$

----

## 多元回归中的一些重要问题

在做多元回归的时候我们经常会面对如下一些问题：

1. 预测变量中是否是否至少有一个可以用来预测响应变量
2. 所有预测变量都有助于理解 Y 吗？或仅仅是预测变量的一个子集对预测有用？
3. 模型对数据拟合程度如何？
4. 给定一组预测变量的值，响应值应预测为多少？多做预测的准确程度如何。

### F test

如我们在一元回归中要验证原假设 $\beta_1 = 0$ 一样 （在一元线性回归中由于只有一个系数，因此模型的 F 检验等于等同于相应系数的 t 检验）多元回归中模型的原假设则为 $\beta_1=\beta_2=\dots=\beta_p=0$. 备择假设为：至少有一个 $\beta_j$ 不为零。
  
同样我们需要计算 F 统计量：

$$
F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}
$$

如果原假设正确，则：

$$
E\{RSS/(n-p-1)\}=\sigma^2\\
E\{(TSS-RSS)/p\}=\sigma^2
$$

因此，若预测变量与响应变量无关时，F 统计量应该是接近 1 的。在误差项  $\sigma_j$ 满足正态分布的时候，F 统计量满足 F 分布。我们可以通过对应的 p 值来做判断。

问题一：一些预测变量单独和响应变量建立线性模型时，会得到一个较小的 p 值。但是加入其他变量后，反而会得到一个较大的 p 值。这似乎有些违背我们的直觉。

首先，F 统计量给我们提供了，预测变量中是否至少有一个变量可以用来预测响应变量的判断依据。但是有时我们想检验的是特定的子集。
  
事实上每个变量的 t 检验都等价于不含该变量，但包含其他变量模型的 F 检验（t 值的平方即 F 统计量）。当子集内变量个数为 1 的时候，其反映了该变量加入模型所产生的偏效应(partial effect)。这些统计量提供的信息表明了在控制所有其他预测变量之后，每个预测变量与响应变量之间的关系。

因此，对于上面的问题的合理解释是，没有任何证据表明当**其他变量加入模型后**，其与响应变量相关。


问题二：既然能得到所有的预测变量的 p 值，为什么还要看整体的F 统计量呢？似乎如果有一个预测变量的 p 值很小，那么至少有一个预测变量与响应变量相关。
  
事实上这种逻辑是有缺陷的，特别是变量数目很大的时候。



（偏效应也就是指某自变量（解释变量）具有对因变量（被解释变量）在其他情况不变条件下的解释的效应。参考来源：伍德里奇《计量经济学导论》第四版。P70）

























